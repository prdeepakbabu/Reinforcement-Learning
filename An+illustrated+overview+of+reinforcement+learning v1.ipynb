{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"iisc_logo.png\"  width=80 height=80>\n",
    "<center><h1>An illustrated Overview of Reinforcement Learning</h1>\n",
    "<small>Deepak Babu P R,Sr. Data Scientist <br>\n",
    "Prof. Shalabh Bhatnagar, IISc - Indian Institute of Science.</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) is a learning paradigm different from traditional machine learning (supervised and unsupervised). The learning problem considered here mimics humans learning from interactions using trial-and-error and has historically been used in the context of planning/decision making related problems like robotics and autonmous driving. We can apply RL in cases where there is a need to make sequential decisions to optimize a metric. Let's consider an example of news recommendation problem. Users are presented list of articles that are of potential interest to them and user decides to either click(and read) vs. not-click. How do we(called agent) organize or rank the articles to maximize click-thru rates(CTR)? Is there a strategy(called policy in RL terminology) that leads to better relevance and hence improve CTR ? Using supervised learning for this problem would attempt to learn the P(click|article) independently for all articles. With RL, at each position we are trying to learn P(click|article1, article2, article3) considering the sequence of articles already seen by the user. We are maximizing for total future cumulative rewards(clicks) that we expect to recieve by recommending this article given the fact that the user has already seen article 1,2 and 3. Intuitively, it makes sense to consider the sequence of articles already seen because 100s of articles might be relevant to a user across various topics and if we end up showing articles from one topic we might risk user getting fatigue and quiting the page. Instead, RL tries to arrive at the optimal policy that balances diversity in topics that are of interest to the user maximizing potential future clicks from that user <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Today, RL has been used in a couple of industries with varied success and hasnt yet become mainstream technique yet. RL has been applied to game playing with good success, one of the early wins being game of <a  href='https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf' target=\"_blank\">AlphaGo</a> in which a RL agent beats expert human player. Consumer Internet, Quantitative Trading, Resource management, Advertising and Manufacturing are other areas where RL has been successfully applied. One of the reason RL has been extensively applied to game playing is because of our ability to collect lots of data through simulation of the game. With real-world problems, the data collection is far more expensive and in some cases infeasible which is one of the reasons preventing wide-spread adoption of RL.<br>\n",
    "<img src=\"env.png\" style=\"float:right\" height=200 width=400>\n",
    "       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Next, we consider the mathematical representation of Reinforcement Learning problem called Markov Decision Process(MDP).MDPs are a classical formalization of sequential decision making,\n",
    "where actions influence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to tradeoff immediate and delayed reward.An MDP involves following components<br>\n",
    "<ul>\n",
    "<li><strong>Agent</strong> is learner and decision maker.</li>\n",
    "<li><strong>Environment</strong> comprises everything the agent interacts with. </li>\n",
    "<li>Everytime agent interacts with environment by taking <strong>actions</strong>, the environment responds by changing its <strong>state</strong></li>\n",
    "<li>For every action taken, the agent recieves a scalar <strong>reward</strong> indicating goodness of action taken. Reward can be instantaneous or delayed. </li>\n",
    "The MDP and agent together thereby give rise to a sequence or trajectory that begins like this<br>\n",
    "<center>S0,A0,R1, S1,A1,R2, S2,A2,R3, . . .</center>\n",
    "Below are some important considerations while designing a RL problem. Evaluating each of these questions will narrow down the solution framework for the problem. \n",
    "<ul>\n",
    "<li>What are the states and actions in my env ?</li>\n",
    "<li>Are my interactions with environment continous and infinite or Is there a natural end state for interactions? <mark> <ins> continous vs. episodic</ins> </mark> | <mark><ins>discounted vs. non-discounted</ins>  </mark></li>\n",
    "<li>Are there infinitely many (large number) of states ? <mark><ins>Tabular vs. Approximate methods</ins> </mark> </li> \n",
    "<li>Do we know the system model ? or Do we know how my environment behaves to actions in different states ?><mark><ins>model-based(DP) vs. model-free(MC,TD)</ins> </mark></li>\n",
    "<li>Are the rewards delayed or instantaneous ? Does agent involve trial-and-error actions ? Is there a sequential decision making ?<mark><ins> supervised learning vs. reinforcement learning</ins></mark></li>\n",
    "<li>How should I design my reward scheme ? what metric should I base my rewards upon ?</li>\n",
    "<li>How far-sighted should your agent be? <mark><ins>choice of discount factor</ins> </mark></li>\n",
    "<li>Does the agent learn and act on the strategy for taking actions(policy) in environment simultaneously ?<mark><ins>on-policy vs. off-policy</ins> </mark>\n",
    "<li>Are your states complex to represent/featurize? <mark>mostly <ins>Deep Reinforcement Learning</ins></mark></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "<strong><img src=\"windygrid_some.png\" style=\"float:right\" width=400 height=200>Windy Grid World</strong> is a modification to the famous grid world problem. The problem concerns navigating a grid of 7 X 10 cells by taking a sequence of actions(UP,DOWN,LEFT,RIGHT) at each cell. The goal is to reach the end state/cell - (3,7).An action from a state that leads to (3,7) gets a reward of 0 and every other transition results in a reward of -1. We have additional constraint of wind: There is wind blowing upwards in the columns 4-9 and the wind shift the agent by +1 or +2 steps depending on column the agent moved. Find the optimal path (least cost) path possible to reach (3,7) from (3,0). <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We will use python <a href='https://gym.openai.com/' target='_blank'>openAI gym</a> for simulating the windygrid environment. Gym is a RL toolkit by open AI which provides hundreds of environments for researchers and developers to develop novel RL algorithms. Each environment is a definition of dynamics of a system which can be used for simulation with option for visualizing the environment. Enviroment provides ways to record rewards from taking actions in a state. In our case, resetting the environment takes the states to (3,0), our start state. The target state is (3,7). We have 70 possible states the agent can be in at anytime. Each state is represented as (row,column) index and stored as 2D arrays. In every state, there are 4 possible actions (0-UP, 1-RIGHT, 2-DOWN, 3-LEFT)\n",
    "       </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the current literature of RL focus on different problems to demonstrate the various RL techniques which makes comparison harder. To address, we will use the windy grid world to demostrate DP, MC and TD based algorithms. We will learn the Q function and V function along with optimal policy pi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the current literature of RL focus on different problems to demonstrate the various RL techniques which makes comparison harder. To address, we will use the windy grid world to demostrate DP, MC and TD based algorithms. We will learn the Q function and V function along with optimal policy pi*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run helper.py\n",
    "import gym\n",
    "import gym_gridworlds\n",
    "env = gym.make('WindyGridworld-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dynamic Programming </h3>\n",
    "DP is a model-based RL technique which assumes the complete knowledge of the system is readily available. i.e transition probabilities and Rewards/incentive scheme. Below code, we try to solve the Bellman Equation iteratively using Generailzed Policy Iteration. We start with random policy and evaluate wrt. to it. Based on value function learnt as part of evaluation, we will update the policy to being greedy wrt. value function. Next, we again evaluate the new policy to get an updated value function and update the policy to be greedy wrt current value function and this proceeds recursively until policy doesnt change(we call this convergence criteria). At this time, we have optimal policy and hence can evalaute once to get optimal value function and optimal action-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged after  127  iterations\n"
     ]
    }
   ],
   "source": [
    "    import numpy as np  \n",
    "    #initialize\n",
    "    policy = np.ones([70, 4]) / 4\n",
    "    V = np.zeros(70)\n",
    "    Q = np.zeros([70,4])\n",
    "        \n",
    "    #step1 : policy evaluation(complete)\n",
    "    overall = 0 \n",
    "    while True :\n",
    "        cnt = 0\n",
    "        while True:\n",
    "            gamma = 1\n",
    "            cnt = cnt + 1\n",
    "            #theta = 1e-8\n",
    "            theta = 0.01\n",
    "            delta = 0\n",
    "            oldpolicy = policy\n",
    "            Vtmp = np.zeros(70)\n",
    "            for s in range(70):\n",
    "                ## store old value of V[s] to calculate delta\n",
    "                v = V[s]\n",
    "                v_new = 0\n",
    "                for a in range(4):\n",
    "                    env.S = (s/10,s%10)\n",
    "                    ## Iterate over possible results of taking action a\n",
    "                    ns,r,done, info = env.step(a)\n",
    "                    if ns == (3,7):\n",
    "                        r = 0\n",
    "                    v_new += policy[s][a]*(r+gamma*V[10 * ns[0] + ns[1]])\n",
    "                delta = max(delta, abs(v-v_new))\n",
    "                Vtmp[s] = v_new\n",
    "            #print delta\n",
    "            V =  Vtmp\n",
    "            if delta < theta or cnt == 1:\n",
    "                break\n",
    "        \n",
    "        #step2 :Derive Q from V\n",
    "        for s in range(70):\n",
    "            for a in range(4):\n",
    "                q_a = 0\n",
    "                env.S = (s/10,s%10)\n",
    "                ns,r,done, info = env.step(a)\n",
    "                if ns == (3,7):\n",
    "                    r = 0\n",
    "                q_a += (r + gamma*V[10 * ns[0] + ns[1]])\n",
    "                Q[s][a] = q_a  \n",
    "                \n",
    "        #step 3: policy improvement\n",
    "        policy = np.zeros([70, 4]) / 4   \n",
    "        for s in range(70):\n",
    "            Q_s = Q[s]\n",
    "            argmax = np.argwhere(Q_s == np.amax(Q_s)).flatten().tolist()\n",
    "            ## Create Stochastic policy if multiple actions yield best results\n",
    "            prob = 1.0/len(argmax)\n",
    "            for index in argmax:\n",
    "                policy[s][index] = prob\n",
    "        \n",
    "        #step 4: check if policy converged\n",
    "        if np.array_equal(oldpolicy,policy):\n",
    "            print \"converged after \",overall,\" iterations\"\n",
    "            break\n",
    "        else:\n",
    "            None #print \"iterate\"\n",
    "            overall = overall + 1\n",
    "        oldpolicy = policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"policyiter.png\" style=\"float:right\" width=600 height=300>\n",
    "The above code accomplishes the following steps (iteratively approaching optimality)<br>\n",
    "<strong>step-0 </strong>: Initialization of policy to be equiprobable (ie. in each state the policy mentions taking any action with same prob.), value-function(V) is set to 0. action-value function(Q) is set to 0.<br>\n",
    "<strong>step-1 </strong>: Policy Evaluation(PE) tries to arrive at as estimate of value function under the assumption of policy initialized.Vπ(s)=∑aπ(s,a)∑s′Pass′[Rass′+γVπ(s′)]<br>\n",
    "<strong>step-2 </strong>: Get Q-function from V-function.Qπ(s,a)=Eπ{rt+γVπ(st+1)∣st=s,at=a}=∑s′Pass′[Rass′+γVπ(s′)]<br>\n",
    "<strong>step-3 </strong>: Policy Improvement(PI) updates the policy greedily based on value-function(V and Q) estimated as part of step-1 and step-2.π′(s)=argmaxaQ(s,a)<br>\n",
    "<strong>step-4 </strong>: Check for convergence - Does the policy (prev iter) == policy (current) ? If yes, stop the loop and call it a day ! converged. If not,go back to step and repeat until convergence.\n",
    "\n",
    "\n",
    "Below code visualizes the optimal policy by showing optimal path to be followed from start to end(textually and visually). Total cost incurred is also shown. NOTE: The arrows might look wierd, but do note that there is the effect of wind acting upwards which makes the agent shift more than one cells at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5) => (4, 6) => (2, 7) => (0, 8) => (0, 9) => (1, 9) => (2, 9) => (3, 9) => (4, 9) => (4, 8) => (3, 7)\n",
      "-10 10\n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮞ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮞ ⏹ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ✌ ⏹ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮞ ⏹ ⮜ ⮜ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⮞ ⏹ ⏹ ⏹ ⏹ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ \n"
     ]
    }
   ],
   "source": [
    "visualize_path(Q,(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Monte Carlo Technique</h3>\n",
    "It is unrealistic to assume knowledge about dynamics of the system as done in DP. Most of the real world problems, involve unknown system model and we still need to figure out the optimal path. Monte Carlo technique views system model as black-box and instead tries to sample with random initial states and average rewards from there. This is called \"Monte Carlo with Exploring starts\". MC-ES makes two unrealistic asumptions (i) exploring starts (ii) infintie episodes. To address the exploring starts assumption, on-policy and off-policy methods are used which leverage epsilon-soft policy. \n",
    "<br> key considerations<br>\n",
    "1. Episodic Tasks only <br>\n",
    "2. Require large Episodes <br>\n",
    "3. Can be first-visit or every-visit based <br>\n",
    "MC techniques are the most easy and natural ways to observe the rewards by exploring the environment. It uses simple averaging of rewards (w or w/o discounting) to arrive value of state. V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:52: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "observation = env.reset()\n",
    "states = range(0,70,1)\n",
    "actions = [0,1,2,3]\n",
    "total_rewards = np.zeros([70, env.action_space.n])\n",
    "epi_cnt =  np.zeros([70, env.action_space.n])\n",
    "Q = np.zeros([70, env.action_space.n])\n",
    "Qold = np.zeros([70, env.action_space.n])\n",
    "observation = env.reset()\n",
    "r = 0\n",
    "done = 0\n",
    "epi=0\n",
    "for episode in range(4000): \n",
    "    #current_state = env.reset()\n",
    "    #random exploring starts\n",
    "    current_state = env.reset()\n",
    "    x = random.randint(0,70)\n",
    "    current_state = ((x/10)-1,x%10)\n",
    "    s_a = []\n",
    "    r = []\n",
    "    steps = 0\n",
    "    states = range(0,70,1)\n",
    "    actions = [0,1,2,3]\n",
    "    while not done:\n",
    "            steps = steps + 1\n",
    "            if steps > 800:\n",
    "                states = []\n",
    "                actions = []\n",
    "                break\n",
    "            if random.uniform(0, 1) <= 0.7:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[10 * current_state[0] + current_state[1]])\n",
    "            next_state,reward,done, info = env.step(action)\n",
    "            s_a.append((10 * current_state[0] + current_state[1],action))\n",
    "            current_state = next_state\n",
    "            r.append(reward)   \n",
    "    #udpate reward and states fist-visit\n",
    "    #print steps\n",
    "    for s,a in cartesian([states,actions]):\n",
    "        try:\n",
    "            indx = s_a.index((s,a))\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            continue\n",
    "        cnt = 1\n",
    "        rew = sum(r[indx:])\n",
    "        total_rewards[s,a] = total_rewards[s,a] + rew\n",
    "        epi_cnt[s,a] = epi_cnt[s,a] + 1.0\n",
    "    done = False\n",
    "    Q = (total_rewards/epi_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0) => (3, 1) => (3, 2) => (2, 2) => (2, 3) => (1, 4) => (0, 5) => (0, 6) => (0, 7) => (0, 8) => (0, 9) => (1, 9) => (2, 9) => (3, 9) => (4, 9) => (4, 8) => (3, 7)\n",
      "-16 16\n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⮞ ⮞ ⮞ ⮞ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⮞ ⏹ ⏹ ⏹ ⏹ ⮟ \n",
      "⏹ ⏹ ⮞ ⮞ ⏹ ⏹ ⏹ ⏹ ⏹ ⮟ \n",
      "⮞  ⮞ ⮝ ⏹ ⏹ ⏹ ⏹ ✌ ⏹ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮜ ⮜ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ \n"
     ]
    }
   ],
   "source": [
    "visualize_path(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we start from (4,2). what would be the optimal path ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) => (4, 3) => (2, 3) => (1, 4) => (0, 5) => (0, 6) => (0, 7) => (0, 8) => (0, 9) => (1, 9) => (2, 9) => (3, 9) => (4, 9) => (4, 8) => (3, 7)\n",
      "-14 14\n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⮞ ⮞ ⮞ ⮞ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⮞ ⏹ ⏹ ⏹ ⏹ ⮟ \n",
      "⏹ ⏹ ⏹ ⮞ ⏹ ⏹ ⏹ ⏹ ⏹ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ✌ ⏹ ⮟ \n",
      "⏹ ⏹ ⮞ ⮝ ⏹ ⏹ ⏹ ⏹ ⮜ ⮜ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ \n"
     ]
    }
   ],
   "source": [
    "visualize_path(Q,(4,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Temporal Differencing : Q-learning </h3><br>\n",
    "TD methods attempt to unify key advantages from both DP and MC. It retains advantages of DP (bootstrapping or step-wise update) and idea of sampling from MC. Below we apply an off-policy technique called Q-learning.  On-policy counterpart of Q-learning is called SARSA for state-action-reward-state-action.  The actual learning happens through exploration in state-action space and uses e-greedy algorithm. It is to be noted, the value-action function is updated immediately after each step unlike MC techniques which needed waiting till the end of episode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15 15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "observation = env.reset()\n",
    "Q = np.zeros([70, env.action_space.n])\n",
    "observation = env.reset()\n",
    "r = 0\n",
    "for episode in range(300): \n",
    "    current_state = env.reset()\n",
    "    for step in range(8000):\n",
    "            #print observation\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[10 * current_state[0] + current_state[1]])\n",
    "            # #######print current_state\n",
    "            # print action\n",
    "            next_state,reward,done, info = env.step(action)\n",
    "            Q[10 * current_state[0] + current_state[1],action] = 0.8 *  Q[10 * current_state[0] + current_state[1],action] + 0.2 * (reward + np.max(Q[10 * next_state[0] + next_state[1]])) \n",
    "            current_state = next_state\n",
    "            #print observation, reward, done, info\n",
    "            ######print action,next_state,reward\n",
    "            #r = r + reward\n",
    "            if done:\n",
    "                #print(\"Finished after {} timesteps\".format(t+1))\n",
    "                #print(Q)\n",
    "                break\n",
    "                \n",
    "#total cost to get to the target\n",
    "tot_reward = 0\n",
    "current_state = env.reset()\n",
    "cnt = 0 \n",
    "for step in range(100):\n",
    "    #print current_state\n",
    "    next_action = np.argmax(Q[10 * current_state[0] + current_state[1]])\n",
    "    next_state,reward,done, info = env.step(next_action)\n",
    "    tot_reward = tot_reward + reward\n",
    "    current_state = next_state\n",
    "    cnt = cnt + 1\n",
    "    if done:\n",
    "        #print(\"Finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "print tot_reward,cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0) => (3, 1) => (3, 2) => (3, 3) => (2, 4) => (1, 5) => (0, 6) => (0, 7) => (0, 8) => (0, 9) => (1, 9) => (2, 9) => (3, 9) => (4, 9) => (4, 8) => (3, 7)\n",
      "-15 15\n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮞ ⮞ ⮞ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⮞ ⏹ ⏹ ⏹ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⮞ ⏹ ⏹ ⏹ ⏹ ⮟ \n",
      "⮞  ⮞ ⮞ ⮞ ⏹ ⏹ ⏹ ✌ ⏹ ⮟ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮜ ⮜ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ \n",
      "⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ \n"
     ]
    }
   ],
   "source": [
    "visualize_path(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the methods discussed so far fall under so called 'tabular methods' which works well in case of finite managable set of states. If we extend the same windy grid problem as being a grid of 10000 X 10000, the tabular methods doesnt scale well. In such scenarios, approximate methods are employed which uses supervised learning to learn value function given a set of features describing the state. If deep NNs are used in value function approximation, then it falls under deep Reinforcement Learning topic.\n",
    "<h3>Conclusion</h3>\n",
    "As seen from simulations and optimal policy derived from DP, MC and TD algorithms, the best strategy to navigate the grid from (3,0) would be to take extreme outer cells and approach round about of target cell (3,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> References </h3><br>\n",
    "<ul>\n",
    "<li><strong><a href='http://incompleteideas.net/book/the-book-2nd.html'>Book : Reinforcement Learning: An Introduction</a>, Second Edition, Feb 2018</strong><br>\n",
    "<small>Richard S. Sutton and Andrew G. Barto</small></li>\n",
    "<li><strong><a href='https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html' target='_blank'>REINFORCEjs : Interactive Visualization of Rl Algorithms</a></strong>\n",
    "<br><small>Andrej Karpathy (@karpathy)</small></li>\n",
    "<li> <strong><a href='https://gym.openai.com/'>RL Algorithms toolkit</a> </strong><br> <small>OpenAI Gym, Python </small></li>\n",
    "<li> <strong> <a href='https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4'>Overview of Reinforcement Learning </a> </strong><br><small>Madhu Sanjeevi</small> </li>\n",
    "<li> <strong><a href='https://towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2'>Introduction: Reinforcement Learning with OpenAI Gym</a></strong> <br><small> Ashish Rana </small></li>\n",
    "<li> <strong><a href='http://karpathy.github.io/2016/05/31/rl/'>Deep Reinforcement Learning: Pong from Pixels</a></strong> <br><small> Andrej Karpathy </small></li>\n",
    "<li> <strong><a href='https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ'>Reinforcement Learning course </a></strong> <br><small> David Silver, DeepMind </small></li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
